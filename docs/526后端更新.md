# 5月26日后端更新文档

## 新增功能概述

### 1. Token计数系统 (`app/utils/token_counter.py`)
- 实现了多种token计数方法：
  - 字符计数（适用于快速估算）
  - 分词计数（使用jieba，适用于中文）
  - tiktoken计数（适用于精确计算）
  - 自动模式（优先使用tiktoken，失败时回退到分词）
- 对应MVP文档中的"文本预处理"部分
- 支持中英文混合文本处理

### 2. 文本处理工具 (`app/utils/text_processor.py`)
- 实现了文本分块功能
  - 支持自定义块大小和重叠度
  - 自动处理中英文混合文本
  - 保留文本块在原文中的位置信息
- 添加了文本块元数据处理
  - 支持文献ID和组ID关联
  - 自动计算文本统计信息
- 对应MVP文档中的"文本分析"和"数据预处理"部分

### 3. 测试套件 (`tests/test_text_processing.py`)
- 完整的单元测试覆盖
- 测试所有主要功能点
- 包含边界条件验证

## 文件结构
```
app/
  └── utils/
      ├── token_counter.py  # Token计数实现
      └── text_processor.py # 文本处理工具
tests/
  └── test_text_processing.py # 测试套件
```

## 依赖更新
- 主要依赖包：
  - langchain>=0.1.0
  - tiktoken>=0.5.0
  - jieba>=0.42.1
  - scikit-learn>=1.0.2

## 模型API接入指南

当获取到大模型API后，需要进行以下修改：

### 1. Token计数调整
在 `token_counter.py` 中：
- 更新 `estimate_tokens_by_tiktoken` 方法中的模型名称
- 根据实际使用的模型调整token计算方式
```python
# 示例修改
def estimate_tokens_by_tiktoken(text: str, model_name: str = "your-model-name"):
    # 更新为实际使用的模型名称
```

### 2. 文本分块优化
在 `text_processor.py` 中：
- 根据模型的最大token限制调整默认的chunk_size
- 根据模型的特性优化分块策略
```python
def split_text_into_chunks(
    text: str,
    chunk_size: int = your_model_max_tokens,  # 更新为模型的实际限制
    chunk_overlap: int = chunk_size // 5  # 建议overlap为chunk_size的20%
)
```

### 3. 新增模型调用
需要新增：
- 模型配置文件
- API调用封装
- 错误处理机制
- 速率限制处理

## 后续优化建议
1. 添加更多边界条件的测试用例
2. 实现文本块的缓存机制
3. 添加批量处理功能
4. 优化token计数性能
5. 添加更多文本预处理选项

## 注意事项
- 当前实现已经考虑了中英文混合场景
- Token计数系统支持自动降级
- 所有功能都有完整的单元测试覆盖
- 代码中包含详细的注释和文档字符串 