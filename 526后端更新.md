# 5月26日后端更新文档

## 一、新增了什么功能？

### 1. 支持更多文件格式
现在系统可以处理以下类型的文献：
- PDF文件（之前就支持）
- Word文档（新增！）
- 网页文件（新增！）

这意味着你现在可以：
- 直接上传Word格式的论文和报告
- 保存网页版的学术文章
- 系统会自动提取文字内容，包括表格中的内容

### 2. 智能分析文本
新增了两个重要功能：

📊 **文本分块功能**
- 可以把长文本自动切分成小段
- 保持段落的完整性，不会断句
- 记录每段文字在原文中的位置
- 方便后续分析和查找

🔢 **智能计数系统**
- 支持中英文混合的文本
- 提供多种计数方式：
  - 快速估算（按字符数）
  - 精确计算（使用专业工具）
  - 中文分词计数（使用结巴分词）
- 自动选择最合适的计数方法

### 3. 完整的测试系统
- 确保所有功能都能正常工作
- 包含各种特殊情况的测试
- 方便发现和修复问题

## 二、文件都放在哪里？

```
app/utils/ 文件夹下：
├── text_extractor.py  → 处理各种文件格式（PDF、Word、网页）
├── text_processor.py   → 文本分析工具（分块、处理）
└── token_counter.py    → 文本计数工具

tests/ 文件夹下：
└── test_text_processing.py → 测试文件
```

## 三、需要安装什么？

系统需要这些新的工具包：
- PyPDF2：处理PDF文件
- python-docx：处理Word文档
- beautifulsoup4：处理网页文件
- jieba：中文分词
- langchain：AI工具集
- tiktoken：文本计数
- scikit-learn：机器学习工具
- lxml：网页解析

## 四、后续还要做什么？

近期计划：
1. 支持更多文件格式（如.txt文件、电子书）
2. 改进网页文章的提取效果
3. 加快处理大文件的速度
4. 增加更多文本处理功能

## 五、注意事项

⚠️ 重要提醒：
1. 上传文件时请确保文件格式正确（PDF/Word/网页）
2. 中英文混合的文本都可以正常处理
3. 如果处理失败，系统会自动尝试其他方法
4. 所有操作都有详细的日志记录，方便排查问题

## 六、等待AI模型接入后需要调整的内容

当我们获得AI模型的API后：
1. 需要设置模型的具体参数
2. 调整文本分块的大小
3. 配置API的调用限制
4. 添加错误处理机制

🔔 特别说明：目前的功能都已经为接入AI模型做好了准备，到时候只需要简单配置就可以使用。 